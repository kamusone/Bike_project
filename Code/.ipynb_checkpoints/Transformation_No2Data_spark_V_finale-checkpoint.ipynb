{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation du dataset No2 Paris par station de mesure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](p_paris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Transformer les données sans faire le melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## ******* 1.1 - charger les package nécessaires et créer une session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\" Analyse de la pollution NO2 Paris\")\n",
    "             .enableHiveSupport()\n",
    "             .getOrCreate()\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ******* 1.2 - Charger le dataset No2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Créer un schéma (en cas de besoin)\n",
    "\n",
    "no2_schema = StructType([StructField(\"date\"   , DateType(),True), \n",
    "                          StructField(\"heure\" , IntegerType(),True),\n",
    "                          StructField(\"EVRY\"  , IntegerType(),True),\n",
    "                          StructField(\"PA01H\" , IntegerType(),True),\n",
    "                          StructField(\"LOGNES\", IntegerType(),True),\n",
    "                          StructField(\"VILLEM\", IntegerType(),True), \n",
    "                          StructField(\"GEN\"   , IntegerType(),True), \n",
    "                          StructField(\"MELUN\" , IntegerType(),True),\n",
    "                          StructField(\"ARG\"   , IntegerType(),True),\n",
    "                          StructField(\"GON\"   , IntegerType(),True),\n",
    "                          StructField(\"DEF\"   , IntegerType(),True),\n",
    "                          StructField(\"A1\"    , IntegerType(),True),\n",
    "                          StructField(\"PA13\"  , IntegerType(),True),\n",
    "                          StructField(\"VERS\"  , IntegerType(),True),\n",
    "                          StructField(\"PA15L\" , IntegerType(),True),\n",
    "                          StructField(\"RUR-SE\", IntegerType(),True),\n",
    "                          StructField(\"PA18\"  , IntegerType(),True),\n",
    "                          StructField(\"CHAMP\" , IntegerType(),True),\n",
    "                          StructField(\"STDEN\" , IntegerType(),True),\n",
    "                          StructField(\"RN6\"   , IntegerType(),True),\n",
    "                          StructField(\"PA04C\" , IntegerType(),True),\n",
    "                          StructField(\"BOB\"   , IntegerType(),True),\n",
    "                          StructField(\"MANT\"  , IntegerType(),True),\n",
    "                          StructField(\"OPERA\" , IntegerType(),True),\n",
    "                          StructField(\"RN2\"   , IntegerType(),True),\n",
    "                          StructField(\"HAUS\"  , IntegerType(),True),\n",
    "                          StructField(\"BONAP\" , IntegerType(),True),\n",
    "                          StructField(\"AUT\"   , IntegerType(),True),\n",
    "                          StructField(\"VITRY\" , IntegerType(),True),\n",
    "                          StructField(\"RUR-SO\", IntegerType(),True), \n",
    "                          StructField(\"RN20\"  , IntegerType(),True),\n",
    "                          StructField(\"BASCH\" , IntegerType(),True),\n",
    "                          StructField(\"PA12\"  , IntegerType(),True),\n",
    "                          StructField(\"NEUIL\" , IntegerType(),True),\n",
    "                          StructField(\"MONTG\" , IntegerType(),True),\n",
    "                          StructField(\"SOULT\" , IntegerType(),True),\n",
    "                          StructField(\"CELES\" , IntegerType(),True),\n",
    "                          StructField(\"ELYS\"  , IntegerType(),True),\n",
    "                          StructField(\"EIFF3\" , IntegerType(),True),\n",
    "                          StructField(\"TREMB\" , IntegerType(),True),\n",
    "                          StructField(\"AUB\"   , IntegerType(),True),\n",
    "                          StructField(\"IVRY\"  , IntegerType(),True),\n",
    "                          StructField(\"BP_EST\", IntegerType(),True),\n",
    "                          StructField(\"PA07\"  , IntegerType(),True)])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF = spark.read.csv(\"/Data/20180101_20201212-NO2_auto.csv\", sep=\",\", header=True)               \n",
    "no2_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******* 1.3 - Supprimer les colonnes des stations qui sont en dehors de paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En se basant sur les adresses des capteurs, récupérés par scrapping sur le site airparif.fr\n",
    "no2_DF_Paris = no2_DF.drop(\"GON\", \"A1\" , \"MANT\" , \"MELUN\", \"TREMB\", \"VERS\", \"RUR-SE\", \"RUR-SO\", \"RN2\"  , \"RN20\" , \"RN6\"   , \"ARG\"  , \"PA04C\"\n",
    "                           \"AUB\", \"BOB\", \"CHAMP\", \"DEF\"  , \"EVRY\" , \"GEN\" , \"LOGNES\", \"MONTG\" , \"NEUIL\", \"STDEN\", \"VILLEM\", \"VITRY\", \"IVRY\")\n",
    " \n",
    "no2_DF_Paris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir le type des colonnes en float :\n",
    "# ceci est nécéssaire pour faire le groupby (pour le calcul de la moyenne)\n",
    "\n",
    "no2_DF_Paris = no2_DF_Paris.withColumn(\"date\"  , col(\"date\"  ).cast(StringType())) \\\n",
    "                           .withColumn(\"hour\"  , col(\"hour\"  ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"PA01H\" , col(\"PA01H\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"PA13\"  , col(\"PA13\"  ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"PA15L\" , col(\"PA15L\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"PA18\"  , col(\"PA18\"  ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"PA04C\" , col(\"PA04C\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"OPERA\" , col(\"OPERA\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"HAUS\"  , col(\"HAUS\"  ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"BONAP\" , col(\"BONAP\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"AUT\"   , col(\"AUT\"   ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"BASCH\" , col(\"BASCH\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"PA12\"  , col(\"PA12\"  ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"SOULT\" , col(\"SOULT\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"CELES\" , col(\"CELES\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"ELYS\"  , col(\"ELYS\"  ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"EIFF3\" , col(\"EIFF3\" ).cast(\"Float\"))      \\\n",
    "                           .withColumn(\"BP_EST\", col(\"BP_EST\").cast(\"Float\"))      \\\n",
    "                           .withColumn(\"PA07\"  , col(\"PA07\"  ).cast(\"Float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le schéma du dataframe\n",
    "no2_DF_Paris.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF_Paris.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la colonne date au format datetime\n",
    "\n",
    "from datetime import datetime\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def user_defined_timestamp(date_col):\n",
    "    _date = datetime.strptime(date_col, '%d/%m/%Y')\n",
    "    return _date.strftime('%Y-%m-%d')\n",
    "\n",
    "user_defined_timestamp_udf = F.udf(user_defined_timestamp, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF_Paris = no2_DF_Paris.withColumn('date', user_defined_timestamp_udf('date'))\n",
    "no2_DF_Paris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter le jour de la semaine\n",
    "no2_DF_Paris = no2_DF_Paris.withColumn('week_day', date_format('date', 'EEEE'))\n",
    "no2_DF_Paris.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonction pour avoir week_day\n",
    "\n",
    "def get_weekday(date):\n",
    "    import datetime\n",
    "    import calendar\n",
    "    day, month, year = (int(x) for x in date.split('-'))    \n",
    "    weekday = datetime.date(year, month, day)\n",
    "    return calendar.day_name[weekday.weekday()]\n",
    "\n",
    "spark.udf.register('get_weekday', get_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changer l'ordre des colonnes\n",
    "\n",
    "no2_DF_Paris = no2_DF_Paris[[\"date\", \"hour\", \"week_day\", \"PA01H\", \"PA13\", \"PA15L\", \"PA18\", \"PA04C\", \"OPERA\", \"HAUS\", \"BONAP\", \"AUT\", \"BASCH\", \"PA12\", \"SOULT\", \"CELES\", \"ELYS\", \"EIFF3\", \"BP_EST\", \"PA07\"]]\n",
    "no2_DF_Paris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifier le format de l'heure\n",
    "\n",
    "no2_DF_Paris_t = no2_DF_Paris.withColumn('hour', regexp_replace('hour', '1.0' , '01:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '2.0' , '02:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '3.0' , '03:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '4.0' , '04:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '5.0' , '05:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '6.0' , '06:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '7.0' , '07:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '8.0' , '08:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '9.0' , '09:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '10.0', '10:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '11.0', '11:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '12.0', '12:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '13.0', '13:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '14.0', '14:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '15.0', '15:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '16.0', '16:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '17.0', '17:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '18.0', '18:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '19.0', '19:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '20.0', '20:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '21.0', '21:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '22.0', '22:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '23.0', '23:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '24.0', '00:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '101:00', '11:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '102:00', '12:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '103:00', '13:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '104:00', '14:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '105:00', '15:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '106:00', '16:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '107:00', '17:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '108:00', '18:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '109:00', '19:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '201:00', '21:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '202:00', '22:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '203:00', '23:00'))\n",
    "no2_DF_Paris_t = no2_DF_Paris_t.withColumn('hour', regexp_replace('hour', '204:00', '00:00'))\n",
    "no2_DF_Paris_t.show(25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff = no2_DF_Paris_t.drop(\"PA04C\", \"PA07\")\n",
    "aff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF_Paris_t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF_Paris_t.select([count(when(col(c).isNull(), c)).alias(c) for c in no2_DF_Paris.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF_Paris_t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *****************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Transformer et pivoter les données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 - Pivoter le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataframe initial avant le groupby\n",
    "\n",
    "columns_no2 = no2_DF_Paris_t.columns[3:]\n",
    " \n",
    "no2_DF_Paris_pivot = no2_DF_Paris_t.selectExpr('date','hour', 'week_day', \"stack({}, {})\".format(len(columns_no2), ', '.join((\"'{}', {}\".format(i, i) for i in columns_no2))))\n",
    "\n",
    "no2_DF_Paris_pivot = no2_DF_Paris_pivot.withColumnRenamed(\"col0\", \"station_code\")\n",
    "no2_DF_Paris_pivot = no2_DF_Paris_pivot.withColumnRenamed(\"col1\", \"taux_no2\")\n",
    "display(no2_DF_Paris_pivot)\n",
    "no2_DF_Paris_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF_Paris_pivot.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - Charger le fichier qui contient les adresses des stations de mesure No2\n",
    "### Ce fichier est le résultat du scrapping du site airparif\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adress_stations = spark.read.csv(\"/Data/stationsNO2_paris.csv\", sep=\",\", header = True)               \n",
    "adress_stations.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adress_stations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Join le fichier adress et fichier No2 avant le groupby\n",
    "\n",
    "joined_no2_adress =  no2_DF_Paris_pivot.join(adress_stations, 'station_code', \"inner\")\n",
    "joined_no2_adress.show()\n",
    "\n",
    "pour_affich = joined_no2_adress.drop(\"station_code\")\n",
    "pour_affich.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_DF_Paris_pivot.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_no2_adress.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "joined_no2_adress.select([count(when(col(c).isNull(), c)).alias(c) for c in joined_no2_adress.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_no2_adress.coalesce(1).write \\\n",
    ".option(\"header\",True) \\\n",
    ".csv(\"../resultat_merge_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coalesce(1) ou repartition(1) --> exporter les resultats en 1 seul fichier csv\n",
    "'''\n",
    "join_no2_adress.coalesce(1).write \\\n",
    ".option(\"header\",True) \\\n",
    ".csv(\"../resultat_merge_spark.csv\")\n",
    "\n",
    "\n",
    "join_no2_adress_h.coalesce(1).write \\\n",
    ".option(\"header\",True) \\\n",
    ".csv(\"../resultat_merge_spark_h.csv\")\n",
    "\n",
    "join_no2_adress_j.coalesce(1).write \\\n",
    ".option(\"header\",True) \\\n",
    ".csv(\"../resultat_merge_spark_j.csv\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Ingestion vers Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "CASSANDRA_HOST = ['localhost']\n",
    "CASSANDRA_PORT = 9042\n",
    "CASSANDRA_DB = \"no2_Paris\"\n",
    "CASSANDRA_TABLE = \"stationsno2_paris\"\n",
    "\n",
    "auth_provider = PlainTextAuthProvider(username='cassandra', password='cassandra')\n",
    "\n",
    "try :\n",
    "    cluster = Cluster(protocol_version=3,auth_provider=auth_provider,contact_points=CASSANDRA_HOST,load_balancing_policy=None ,port=CASSANDRA_PORT)\n",
    "    session =cluster.connect()\n",
    "except ValueError :\n",
    "    print(\"Oops! échec de connexion cluster. Try again...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du key space\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS No2_Paris WITH REPLICATION={'class':'SimpleStrategy','replication_factor':3};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation de la table\n",
    "session.execute(\"CREATE TABLE No2_Paris.stationsNO2_Paris(col_id Text, station_code Text, date Text, hour Text, week_day Text, taux_no2 Text, station_name Text, station_adress Text, gps_lat Text, gps_long Text, zip_code Text,primary key (col_id));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cluster.connect('no2_paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert spark dataframe to pandas\n",
    "\n",
    "no2_pd_dataframe = joined_no2_adress.select(\"*\").toPandas()\n",
    "no2_pd_dataframe[\"taux_no2\"] = no2_pd_dataframe[\"taux_no2\"].fillna(999)\n",
    "no2_pd_dataframe[\"col_id\"] = no2_pd_dataframe.index\n",
    "no2_pd_dataframe = no2_pd_dataframe[[\"col_id\",  \"station_code\", \"date\", \"hour\", \"week_day\", \"taux_no2\", \"station_name\", \"station_adress\", \"gps_lat\", \"gps_long\", \"zip_code\"]]\n",
    "no2_pd_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_pd_dataframe[\"col_id\"] = no2_pd_dataframe[\"col_id\"].astype(str)\n",
    "no2_pd_dataframe[\"taux_no2\"] = no2_pd_dataframe[\"taux_no2\"].astype(str)\n",
    "no2_pd_dataframe[\"zip_code\"] = no2_pd_dataframe[\"zip_code\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_pd_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_insert=\"INSERT INTO no2_Paris.stationsno2_paris (col_id, station_code, date, hour, week_day, taux_no2, station_name, station_adress, gps_lat, gps_long, zip_code) VALUES ($${}$$, '{}','{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}');\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct in no2_pd_dataframe.index:  #\n",
    "    CQL_query = query_insert.format(no2_pd_dataframe['col_id'][ct]  , no2_pd_dataframe['station_code'][ct],no2_pd_dataframe['date'][ct], no2_pd_dataframe['hour'][ct], no2_pd_dataframe['week_day'][ct] , no2_pd_dataframe['taux_no2'][ct], no2_pd_dataframe['station_name'][ct], no2_pd_dataframe['station_adress'][ct],no2_pd_dataframe['gps_lat'][ct],no2_pd_dataframe['gps_long'][ct], no2_pd_dataframe['zip_code'][ct],'{}')\n",
    "    print(CQL_query)\n",
    "    session.execute(CQL_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = session.execute('SELECT * FROM no2_paris.stationsno2_paris;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df=spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"stationsno2_paris\", keyspace=\"no2_paris\").load()\n",
    "#no2_DF_Paris_pivot.write.format(\"org.apache.spark.sql.cassandra\").mode('append').option(\"confirm.truncate\",\"true\").options(table=\"stationsno2_paris\", keyspace=\"no2_paris\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation du dataset Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF = spark.read.csv(\"../traffic_transf_pandas.csv\", sep=\",\", header = True)               \n",
    "Traffic_DF.show(2)\n",
    "Traffic_DF.select(\"sensor_id\", \"sensor_name\", \"site_id\", \"trafic_per_hour\",  \"latitude\", \"longitude\", \"hour\", \"date\", \"zip_code\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF = Traffic_DF.withColumn(\"day_of_week\", date_format('date', 'EEEE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF.coalesce(1).write \\\n",
    ".option(\"header\",True) \\\n",
    ".csv(\"../Traffic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF = Traffic_DF[[\"sensor_id\", \"clean_sensor_name\", \"sensor_name\" , \"site_id\", \"site_name\", \"street_name\", \"gps\", \"latitude\", \"longitude\", \"zip_code\", \"trafic_per_hour\", \n",
    "                         \"recording_date\", \"date\", \"hour\", \"day_of_week\", \"sensor_installation_date\", \"orientation\", \"picture_link\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF = Traffic_DF.withColumn(\"trafic_per_hour\"  , col(\"trafic_per_hour\").cast(\"float\"))\n",
    "Traffic_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by trafic\n",
    "\n",
    "Traffic_DF_gpd = Traffic_DF.groupby(['zip_code',\"date\",\"hour\"]).agg(avg(\"trafic_per_hour\"))\n",
    "Traffic_DF_gpd.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF_gpd = Traffic_DF_gpd.withColumnRenamed(\"avg(trafic_per_hour)\", \"trafic_per_hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF_gpd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "Traffic_DF_gpd = Traffic_DF_gpd.select(\"*\").withColumn(\"col_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF_gpd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF_gpd = Traffic_DF_gpd.withColumn('recording_date',concat(\" \", \"date\", \"hour\"))\n",
    "Traffic_DF_gpd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## convert spark dataframe to pandas\n",
    "\n",
    "trafic_pd_dataframe = Traffic_DF_gpd.select(\"*\").toPandas()\n",
    "trafic_pd_dataframe = trafic_pd_dataframe[[\"col_id\", \"date\", \"hour\", \"trafic_per_hour\", \"zip_code\", \"recording_date\"]]\n",
    "trafic_pd_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafic_pd_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafic_pd_dataframe[\"col_id\"] = trafic_pd_dataframe[\"col_id\"].astype(str)\n",
    "trafic_pd_dataframe[\"avg(trafic_per_hour)\"] = trafic_pd_dataframe[\"trafic_per_hour\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafic_pd_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du key space\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS trafic_Paris WITH REPLICATION={'class':'SimpleStrategy','replication_factor':3};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation de la table\n",
    "session.execute(\"CREATE TABLE trafic_Paris.trafic_velo_paris(col_id Text, date Text, hour Text, trafic_per_hour Text, zip_code Text, recording_date Text, primary key (col_id));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cluster.connect('trafic_paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_insert_trafic = \"INSERT INTO no2_paris.trafic_velo_paris(col_id, date, hour, trafic_per_hour, zip_code, recording_date) VALUES ($${}$$, '{}','{}', '{}', '{}', '{}');\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafic_pd_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct in trafic_pd_dataframe.index:  #\n",
    "    CQL_query = query_insert_trafic.format(trafic_pd_dataframe['col_id'][ct], trafic_pd_dataframe['date'][ct], trafic_pd_dataframe['hour'][ct], trafic_pd_dataframe['trafic_per_hour'][ct] , trafic_pd_dataframe['zip_code'][ct], trafic_pd_dataframe['recording_date'][ct],'{}')\n",
    "    print(CQL_query)\n",
    "    session.execute(CQL_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***************   Test ingestion cassandra ********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName('Ingestion NO2 data') \\\n",
    "  .config('spark.cassandra.connection.host', 'localhost') \\\n",
    "  .config('spark.cassandra.connection.port', '9042') \\\n",
    "  .config(\"spark.cassandra.auth.username\",\"cassandra\")\\\n",
    "  .config(\"spark.cassandra.auth.password\",\"cassandra\")\\\n",
    "  .master('local[2]') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_DF_gpd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingest = Traffic_DF_gpd.select(col(\"zip_code\").alias(\"zip_code\"), col(\"date\").alias(\"date\"), col(\"hour\").alias(\"hour\"), col(\"trafic_per_hour\").alias(\"trafic_per_hour\"), col(\"col_id\").alias(\"col_id\"), col(\"recording_date\").alias(\"recording_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingest = data_ingest.withColumn(\"zip_code\"  , col(\"zip_code\"  ).cast(StringType())) \\\n",
    "                           .withColumn(\"date\" , col(\"date\" ).cast(StringType()))  \\\n",
    "                           .withColumn(\"hour\" , col(\"hour\" ).cast(StringType()))  \\\n",
    "                           .withColumn(\"trafic_per_hour\" , col(\"trafic_per_hour\" ).cast(StringType()))  \\\n",
    "                           .withColumn(\"col_id\" , col(\"col_id\" ).cast(StringType()))  \\\n",
    "                           .withColumn(\"recording_date\" , col(\"recording_date\" ).cast(StringType()))  \n",
    "data_ingest.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import org.apache.spark.sql.cassandra\n",
    "data_ingest.write.format(\"org.apache.spark.sql.cassandra\").mode('append').option(\"confirm.truncate\",\"true\").options(table=\"trafic_velo_paris\", keyspace=\"trafic_paris\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
